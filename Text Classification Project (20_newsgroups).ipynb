{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import below modules\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download and extract the files from given link\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/20_newsgroups.tar.gz\", \"a.tar.gz\")\n",
    "import tarfile\n",
    "tar = tarfile.open(\"a.tar.gz\")\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a list of all the file path and their corresponding classes\n",
    "f_paths=[]\n",
    "i=-1\n",
    "path=\"20_newsgroups\"\n",
    "folderlist=listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19997"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if \".DS_Store\" in folderlist:\n",
    "    folderlist.remove('.DS_Store')\n",
    "for folder in folderlist:\n",
    "    i+=1\n",
    "    filelist=listdir(path+'/'+folder)\n",
    "    for file in filelist:\n",
    "        f_paths.append((path+'/'+folder+'/'+file,i))\n",
    "len(f_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14997, 5000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split the file paths into training and testing data\n",
    "from sklearn import model_selection\n",
    "x_train, x_test = model_selection.train_test_split(f_paths)\n",
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the list X_train and X_test that contains the file path for training and testing data respectively\n",
    "X_train=[]\n",
    "X_test=[]\n",
    "Y_train=[]\n",
    "Y_test=[]\n",
    "for i in range(len(x_train)):\n",
    "    X_train.append(x_train[i][0])\n",
    "    Y_train.append(x_train[i][1])\n",
    "for i in range(len(x_test)):\n",
    "    X_test.append(x_test[i][0])\n",
    "    Y_test.append(x_test[i][1])\n",
    "    \n",
    "#convert Y_train and Y_test into one dimensional np array\n",
    "Y_train=(np.array([Y_train])).reshape(-1)\n",
    "Y_test=(np.array([Y_test])).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14997,), (5000,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size of Y_train and Y_test np arrays\n",
    "Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import module to download stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop=set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding all the lists including punctuations to stopwords\n",
    "stop_words=list(stop)+list(set(string.punctuation))\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making vocabulary from the X_train file i.e Training Data\n",
    "vocab={}\n",
    "count=0\n",
    "for filename in X_train:\n",
    "    count+=1\n",
    "    f = open(filename, 'r', errors='ignore')\n",
    "    record=f.read()\n",
    "    words=record.split()\n",
    "    for word in words:\n",
    "        if len(word)>2:\n",
    "            if word.lower() not in stop_words:\n",
    "                if word.lower() in vocab:\n",
    "                    vocab[word.lower()]+=1\n",
    "                else:\n",
    "                    vocab[word.lower()]=1\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352801"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#length of the vocabulary\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the vocabulary on the basis of frequency of the words\n",
    "sorted_vocab = sorted(vocab.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a list feature_names containing top 2000 words with their frequency\n",
    "feature_names=[]\n",
    "for i in range (len(sorted_vocab)):\n",
    "    if(sorted_vocab[2000][1]<=sorted_vocab[i][1]):\n",
    "        feature_names.append(sorted_vocab[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010\n"
     ]
    }
   ],
   "source": [
    "#number of features\n",
    "print(len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dataframes df_train and df_test with columns having the feature names\n",
    "df_train = pd.DataFrame(columns=feature_names)\n",
    "df_test = pd.DataFrame(columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14997 5000\n"
     ]
    }
   ],
   "source": [
    "count_train, count_test = 0,0\n",
    "\n",
    "#transform each file in X_train into a row in the dataframe df_train \n",
    "#having columns as feature names and values as the frequency of that word\n",
    "for filename in X_train:\n",
    "    count_train+=1\n",
    "    df_train.loc[len(df_train)] = np.zeros(len(feature_names))\n",
    "    f = open(filename, 'r', errors='ignore')\n",
    "    record=f.read()\n",
    "    words=record.split()\n",
    "    for word in words:\n",
    "        if word.lower() in df_train.columns:\n",
    "            df_train[word.lower()][len(df_train)-1]+=1\n",
    "    f.close()\n",
    "    \n",
    "#transform each file in X_test into a row in the dataframe df_train \n",
    "#having columns as feature names and values as the frequency of that word    \n",
    "for filename in X_test:\n",
    "    count_test+=1\n",
    "    df_test.loc[len(df_test)] = np.zeros(len(feature_names))\n",
    "    f = open(filename, 'r', errors='ignore')\n",
    "    record=f.read()\n",
    "    words=record.split()\n",
    "    for word in words:\n",
    "        if word.lower() in df_test.columns:\n",
    "            df_test[word.lower()][len(df_test)-1]+=1\n",
    "    f.close()\n",
    "    \n",
    "#print the number files that has been transformed into training and testing data\n",
    "print(count_train, count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the values of df_train and df_test into X_train and X_test\n",
    "X_train = df_train.values\n",
    "X_test = df_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the Inbuilt Multinomial classifier from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.74      0.72       268\n",
      "           1       0.81      0.78      0.80       262\n",
      "           2       0.88      0.83      0.86       235\n",
      "           3       0.84      0.85      0.85       245\n",
      "           4       0.90      0.91      0.90       254\n",
      "           5       0.91      0.86      0.89       264\n",
      "           6       0.63      0.92      0.74       225\n",
      "           7       0.81      0.90      0.85       242\n",
      "           8       0.83      0.95      0.88       251\n",
      "           9       0.96      0.95      0.95       272\n",
      "          10       0.98      0.93      0.95       256\n",
      "          11       0.95      0.88      0.91       226\n",
      "          12       0.78      0.87      0.82       242\n",
      "          13       0.94      0.81      0.87       275\n",
      "          14       0.93      0.90      0.91       270\n",
      "          15       0.95      0.98      0.97       249\n",
      "          16       0.75      0.81      0.78       256\n",
      "          17       0.96      0.78      0.87       246\n",
      "          18       0.71      0.54      0.61       228\n",
      "          19       0.53      0.47      0.50       234\n",
      "\n",
      "    accuracy                           0.83      5000\n",
      "   macro avg       0.84      0.83      0.83      5000\n",
      "weighted avg       0.84      0.83      0.83      5000\n",
      "\n",
      "Testing:  0.835\n"
     ]
    }
   ],
   "source": [
    "#Use MultiNomial classifier from sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "clf=MultinomialNB()\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred=clf.predict(X_test)\n",
    "#print classification report\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "#print testng score\n",
    "print(\"Testing: \", clf.score(X_test, Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
